
# Differentiable Programming

This document provides an overview of our research focus, ongoing projects, future directions, and resources, particularly concerning our work in **Differentiable Programming (DP)**.


## Table of Contents

* [Introduction: Our Focus](#introduction-our-focus)
* [Current Projects](#current-projects)
* [Roadmap and Future Work](#roadmap-and-future-work)
* [Getting Started](#getting-started)
* [Resources](#resources)

## Introduction: Our Focus

### What is Differentiable Programming (DP)?

Differentiable Programming extends the concept of automatic differentiation (AD), popularized by deep learning frameworks, to more general code structures. It allows us to compute gradients of arbitrary programs, enabling optimization and learning in complex systems often involving simulations, physics engines, rendering, or traditional algorithms.

### What We Focus On?

* **Currently:** We are applying DP to general CS problems such as sorting algorithms (bubble sort).
* **Future:** We want to apply DP to LLM code generation. A lot of code, especially complex code generated by LLMs, does not give the correct result. Fine-tuning the LLM in such cases is difficult due to lack of gradients. This project will build on the project above, and will aim to fine-tune LLMs to generate the correct code using differentiable  programming.

## Current Projects

Here are the projects currently active in the lab.

### Project 1: Bubble Sort
* **Description:** Differentiable Bubble Sort
* **Goals:**
    * [x] Create a working bubble sort using differentiable programming components 
    * [ ] Find an appropriate loss function 
* **Status:** Active
* **Repo:** https://github.com/tiwariakshat47/Differentiable-Programming


## Roadmap and Future Work

### Near-Term (Next 3-6 months)
* Finish our bubble sort implementation
* Write a research paper about our findings
* Start working on differentiable programming for LLMs

### Longer-Term / Potential Ideas
* Find out if we can leverage differentiable programming for cryptanalysis (inverting hash functions)

---

## Getting Started

Hereâ€™s how you can get started:

### 1. Prerequisites
* **Technical Skills:** Familiarity with Python is essential. Experience with machine learning libraries is good to have. We are working mostly with [autograd](https://github.com/HIPS/autograd) and [PyTorch](https://github.com/pytorch/pytorch). Basic understanding of calculus (especially derivatives) and linear algebra is really helpful.
* **Conceptual Understanding:** The book ["Elements of differentiable programming"](https://arxiv.org/abs/2403.14606) is a great place to start. Chapters 5 to 10 are good to read before starting on the project.

### 2. Environment Setup
* Please check the project setup guide.


## Resources

### Foundational Concepts & Papers
* **Differentiable Programming:**
    * [Elements of differentiable programming](https://arxiv.org/abs/2403.14606)

### Key Libraries/Frameworks We Use
* [AutoGrad](https://github.com/HIPS/autograd)
* [PyTorch](https://pytorch.org/)

### Other
* [Lab Website](https://razvanmarinescu.com/)
